{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roro\\AppData\\Local\\Temp\\ipykernel_19132\\2876192146.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(\"data/ml-1m/ratings.dat\", sep=\"::\", names=[\"uid\", \"sid\", \"rating\", \"timestamp\"])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/ml-1m/ratings.dat\", sep=\"::\", names=[\"uid\", \"sid\", \"rating\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ratings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>sid</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid   sid  rating  timestamp\n",
       "0    1  1193       5  978300760\n",
       "1    1   661       3  978302109\n",
       "2    1   914       3  978301968\n",
       "3    1  3408       4  978300275\n",
       "4    1  2355       5  978824291"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting ratings to implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000836"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['rating'] >= 0]\n",
    "df.head()\n",
    "df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Triplets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>sid</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid   sid  rating  timestamp\n",
       "0    1  1193       5  978300760\n",
       "1    1   661       3  978302109\n",
       "2    1   914       3  978301968\n",
       "3    1  3408       4  978300275\n",
       "4    1  2355       5  978824291"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_sizes = df.groupby('sid').size()\n",
    "good_items = item_sizes.index[item_sizes >= 0]\n",
    "df = df[df['sid'].isin(good_items)]\n",
    "\n",
    "user_sizes = df.groupby('uid').size()\n",
    "good_users = user_sizes.index[user_sizes >= 5]\n",
    "df = df[df['uid'].isin(good_users)]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Densifying index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>sid</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1104</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>639</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>853</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3177</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2162</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid   sid  rating  timestamp\n",
       "0    0  1104       5  978300760\n",
       "1    0   639       3  978302109\n",
       "2    0   853       3  978301968\n",
       "3    0  3177       4  978300275\n",
       "4    0  2162       5  978824291"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umap = {u: i for i, u in enumerate(set(df['uid']))}\n",
    "smap = {s: i for i, s in enumerate(set(df['sid']))}\n",
    "df['uid'] = df['uid'].map(umap)\n",
    "df['sid'] = df['sid'].map(smap)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Dataframe into train, test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [00:01<00:00, 5321.58it/s]\n"
     ]
    }
   ],
   "source": [
    "user_group = df.groupby('uid')\n",
    "user2items = user_group.progress_apply(lambda x: list(x.sort_values('timestamp')['sid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_count = len(umap)\n",
    "train, val, test = {}, {}, {}\n",
    "for user in range(user_count):\n",
    "    items = user2items[user]\n",
    "    train[user], val[user], test[user] = items[:-2], items[-2:-1], items[-1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': train,\n",
    "    'val': val,\n",
    "    'test': test,\n",
    "    'umap': umap,\n",
    "    'smap': smap\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = '{}_min_rating{}-min_uc{}-min_sc{}-split{}' \\\n",
    "            .format('ml-1m', 3, 5, 0, 'leave_one_out')\n",
    "\n",
    "with open('data/{}.pkl'.format(folder_name), 'wb') as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get negative train samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "dataset_folder = Path(f'data/lstr/seqlen-{max_len}')\n",
    "dataset_filename = '{}_min_rating{}-min_uc{}-min_sc{}-split{}'.format('ml-1m', 3, 5, 0, 'leave_one_out')\n",
    "dataset_file = dataset_folder.joinpath(dataset_filename + '.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset from pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: dict[str, dict[str, list[int]]] = pickle.load(open(dataset_file, 'rb'))\n",
    "train = dataset['train']\n",
    "val = dataset['val']\n",
    "test = dataset['test']\n",
    "umap = dataset['umap']\n",
    "smap = dataset['smap']\n",
    "user_count = len(umap)\n",
    "item_count = len(smap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "\n",
    "def generate_negative_samples(seed: int, sample_size: int = 100) -> dict[str, list[int]]:\n",
    "    np.random.seed(seed)\n",
    "    negative_samples = {}\n",
    "    for user in trange(user_count):\n",
    "        if isinstance(train[user][1], tuple):\n",
    "            seen = set([x[0] for x in train[user]])\n",
    "            seen.update([x[0] for x in val[user]])\n",
    "            seen.update(x[0] for x in test[user])\n",
    "        else:\n",
    "            seen = set(train[user])\n",
    "            seen.update(val[user])\n",
    "            seen.update(test[user])\n",
    "        \n",
    "        samples = []\n",
    "        for _ in range(sample_size):\n",
    "            item = np.random.choice(item_count) + 1\n",
    "            while item in seen or item in samples:\n",
    "                item = np.random.choice(item_count) + 1\n",
    "            samples.append(item)\n",
    "        \n",
    "        negative_samples[user] = samples\n",
    "    \n",
    "    return negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [00:00<00:00, 177632.53it/s]\n",
      "100%|██████████| 6040/6040 [00:06<00:00, 874.73it/s]\n"
     ]
    }
   ],
   "source": [
    "negative_train_samples = generate_negative_samples(0, 0)\n",
    "negative_test_samples = generate_negative_samples(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Negative train samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_test_save_file_path = '{}-sample_size{}-seed{}-{}.pkl'.format('random', max_len, 42, 'test')\n",
    "negative_test_save_file = dataset_folder.joinpath(negative_test_save_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with negative_test_save_file.open('wb') as f:\n",
    "    pickle.dump(negative_train_samples, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with negative_test_save_file.open('rb') as f:\n",
    "    negative_test_samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Coding\\bst-lstr\\bst\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import torch.backends.cudnn as cudnn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTrainDataset(data_utils.Dataset):\n",
    "    def __init__(self, u2seq: dict[str, list[int]], max_len: int, mask_prob: float, mask_token: int, num_items: int, rng: random.Random):\n",
    "        self.u2seq = u2seq\n",
    "        self.users = sorted(self.u2seq.keys())\n",
    "        self.max_len = max_len\n",
    "        self.mask_prob = mask_prob\n",
    "        self.mask_token = mask_token\n",
    "        self.num_items = num_items\n",
    "        self.rng = rng\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user = self.users[index]\n",
    "        seq = self._getseq(user)\n",
    "\n",
    "        tokens: list[int] = []\n",
    "        labels: list[int] = []\n",
    "        for s in seq:\n",
    "            prob = self.rng.random()\n",
    "            if prob < self.mask_prob:\n",
    "                prob /= self.mask_prob\n",
    "\n",
    "                if prob < 0.8:\n",
    "                    tokens.append(self.mask_token)\n",
    "                elif prob < 0.9:\n",
    "                    tokens.append(self.rng.randint(1, self.num_items))\n",
    "                else:\n",
    "                    tokens.append(s)\n",
    "\n",
    "                labels.append(s)\n",
    "            else:\n",
    "                tokens.append(s)\n",
    "                labels.append(0)\n",
    "\n",
    "        tokens = tokens[-self.max_len:]\n",
    "        labels = labels[-self.max_len:]\n",
    "\n",
    "        mask_len = self.max_len - len(tokens)\n",
    "\n",
    "        tokens = [0] * mask_len + tokens\n",
    "        labels = [0] * mask_len + labels\n",
    "\n",
    "        return torch.LongTensor(tokens), torch.LongTensor(labels)\n",
    "\n",
    "    def _getseq(self, user):\n",
    "        return self.u2seq[user]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = BertTrainDataset(train, 100, 0.15, item_count+1, item_count, random.Random(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Dataset (Val and Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEvalDataset(data_utils.Dataset):\n",
    "    def __init__(self, u2seq: dict[str, list[int]], u2answer: dict[str, list[int]], max_len: int, mask_token: int, negative_samples: dict[str, list[int]]) -> None:\n",
    "        self.u2seq = u2seq\n",
    "        self.users = sorted(self.u2seq.keys())\n",
    "        self.u2answer = u2answer\n",
    "        self.max_len = max_len\n",
    "        self.mask_token = mask_token\n",
    "        self.negative_samples = negative_samples\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[torch.LongTensor, torch.LongTensor, torch.LongTensor]:\n",
    "        user = self.users[index]\n",
    "        seq = self.u2seq[user]\n",
    "        answer = self.u2answer[user]\n",
    "        negs = self.negative_samples[user]\n",
    "\n",
    "        candidates = answer + negs\n",
    "        labels = [1] * len(answer) + [0] * len(negs)\n",
    "\n",
    "        seq = seq + [self.mask_token]\n",
    "        seq = seq[-self.max_len:]\n",
    "        padding_len = self.max_len - len(seq)\n",
    "        seq = [0] * padding_len + seq\n",
    "\n",
    "        return torch.LongTensor(seq), torch.LongTensor(candidates), torch.LongTensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_random_seed_as(random_seed: int) -> None:\n",
    "    random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(pl.LightningModule):\n",
    "    def __init__(self, max_len: int, d_model: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.size(0)\n",
    "        return self.pe.weight.unsqueeze(0).repeat(batch_size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    BERT Embedding which is consisted with under features\n",
    "        1. TokenEmbedding : normal embedding matrix\n",
    "        2. PositionalEmbedding : adding positional information using sin, cos\n",
    "        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n",
    "        sum of all these features are output of BERTEmbedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, embed_size: int, max_len: int, dropout: float = 0.1) -> None:\n",
    "        \"\"\"\n",
    "        :param vocab_size: total vocab size\n",
    "        :param embed_size: embedding size of token embedding\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)\n",
    "        self.position = PositionalEmbedding(max_len=max_len, d_model=embed_size)\n",
    "        # self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, sequence: torch.Tensor) -> torch.Tensor:\n",
    "        x: torch.Tensor = self.token(sequence) + self.position(sequence)  # + self.segment(segment_label)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_template(args):\n",
    "    if args.template is None:\n",
    "        return\n",
    "\n",
    "    elif args.template.startswith('train_bert'):\n",
    "        args.mode = 'train'\n",
    "\n",
    "        args.dataset_code = 'ml-' + input('Input 1 for ml-1m, 20 for ml-20m: ') + 'm'\n",
    "        args.min_rating = 0 if args.dataset_code == 'ml-1m' else 4\n",
    "        args.min_uc = 5\n",
    "        args.min_sc = 0\n",
    "        args.split = 'leave_one_out'\n",
    "\n",
    "        args.dataloader_code = 'bert'\n",
    "        batch = 64\n",
    "        args.train_batch_size = batch\n",
    "        args.val_batch_size = batch\n",
    "        args.test_batch_size = batch\n",
    "\n",
    "        args.train_negative_sampler_code = 'random'\n",
    "        args.train_negative_sample_size = 0\n",
    "        args.train_negative_sampling_seed = 0\n",
    "        args.test_negative_sampler_code = 'random'\n",
    "        args.test_negative_sample_size = 100\n",
    "        args.test_negative_sampling_seed = 98765\n",
    "\n",
    "        args.trainer_code = 'bert'\n",
    "        args.device = 'cuda'\n",
    "        args.num_gpu = 1\n",
    "        args.device_idx = '0'\n",
    "        args.optimizer = 'Adam'\n",
    "        args.lr = 0.001\n",
    "        args.enable_lr_schedule = True\n",
    "        args.decay_step = 25\n",
    "        args.gamma = 1.0\n",
    "        args.num_epochs = 100 if args.dataset_code == 'ml-1m' else 200\n",
    "        args.metric_ks = [1, 5, 10, 20]\n",
    "        args.best_metric = 'NDCG@10'\n",
    "\n",
    "        args.model_code = 'bert'\n",
    "        args.model_init_seed = 0\n",
    "\n",
    "        args.bert_dropout = 0.1\n",
    "        args.bert_hidden_units = 256\n",
    "        args.bert_mask_prob = 0.15\n",
    "        args.bert_max_len = 100\n",
    "        args.bert_num_blocks = 2\n",
    "        args.bert_num_heads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import NumberType\n",
    "\n",
    "\n",
    "def recalls_and_ndcgs_for_ks(scores: torch.Tensor, labels: torch.Tensor, ks: list[int]):\n",
    "    metrics: dict[str, NumberType] = {}\n",
    "\n",
    "    # Why is this being done?\n",
    "    # scores = scores\n",
    "    # labels = labels\n",
    "    answer_count = labels.sum(1)\n",
    "\n",
    "    labels_float = labels.float()\n",
    "    rank = (-scores).argsort(dim=1)\n",
    "    cut = rank\n",
    "    for k in sorted(ks, reverse=True):\n",
    "        cut = cut[:, :k]\n",
    "        hits = labels_float.gather(1, cut)\n",
    "        \"\"\"\n",
    "            Recall at k is the proportion of relevant items found in the top-k recommendations.\n",
    "            Recall@k = (# of recommended items @k that are relevant) / (total # of relevant items)\n",
    "        \"\"\"\n",
    "        numerator = hits.sum(1)\n",
    "        denominator = torch.min(torch.Tensor([k]).to(labels.device), labels.sum(1).float())\n",
    "        metrics['Recall@%d' % k] = \\\n",
    "            (numerator / denominator).mean().cpu().item()\n",
    "\n",
    "        \"\"\"\n",
    "            NDCG at k is the average of the normalized DCG scores of the top-k recommendations.\n",
    "            NDCG@k = DCG@k / IDCG@k\n",
    "            DCG@K = SUM( recc_i / log2(i + 1))\n",
    "            IDCG@K = SUM( real_i / log2(i + 1))\n",
    "        \"\"\"\n",
    "        position = torch.arange(2, 2+k)\n",
    "        weights = 1 / torch.log2(position.float())\n",
    "        # Discounted cumulative gain at k\n",
    "        dcg = (hits * weights.to(hits.device)).sum(1)\n",
    "        # Ideal discounted cumulative gain at k\n",
    "        # idcg = (labels_float * weights.to(labels_float.device)).sum(1)\n",
    "        idcg = torch.Tensor([weights[:min(int(n), k)].sum()\n",
    "                            for n in answer_count]).to(dcg.device)\n",
    "        # What is the above code doing\n",
    "        ndcg = (dcg / idcg).mean()\n",
    "        metrics['NDCG@%d' % k] = ndcg.cpu().item()\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(pl.LightningModule):\n",
    "    def __init__(self, args: dotdict):\n",
    "        super().__init__()\n",
    "\n",
    "        fix_random_seed_as(args.model_init_seed)\n",
    "        # self.init_weights()\n",
    "\n",
    "        self.args = args\n",
    "        max_len: int = args.bert_max_len\n",
    "        num_items: int = args.num_items\n",
    "        n_layers: int = args.bert_num_blocks\n",
    "        self.heads: int = args.bert_num_heads\n",
    "        vocab_size: int = num_items + 2\n",
    "        hidden: int = args.bert_hidden_units\n",
    "        self.hidden = hidden\n",
    "        self.metric_ks = args.metric_ks\n",
    "        dropout: float = args.bert_dropout\n",
    "        self.ce = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbedding(\n",
    "            vocab_size=vocab_size, embed_size=self.hidden, max_len=max_len, dropout=dropout)\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.encoder = nn.TransformerEncoderLayer(hidden, self.heads, hidden * 4, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder, n_layers)\n",
    "        self.out = nn.Linear(self.hidden, num_items + 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        mask = (x == 0).unsqueeze(1).repeat(self.heads, x.size(1), 1)\n",
    "        x = self.embedding(x)\n",
    "        output: torch.Tensor = self.transformer(x, mask)\n",
    "        # output = output.masked_fill(torch.isnan(output), 0)\n",
    "        return self.out(output)\n",
    "\n",
    "    def init_weights(self):\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch: tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        batch_size = batch[0].size(0)\n",
    "        seq, labels = batch\n",
    "        logits: torch.Tensor = self(seq)\n",
    "\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        labels = labels.view(-1)\n",
    "        loss: torch.Tensor = self.ce(logits, labels)\n",
    "        # log_data = {\n",
    "        #     'state_dict': (self._create_state_dict()),\n",
    "        #     \"train/ce\": loss.item()\n",
    "        # }\n",
    "        self.log(\n",
    "            \"train/ce\", loss, on_step=True, on_epoch=False, prog_bar=True\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: tuple[torch.Tensor, torch.Tensor, torch.Tensor], batch_idx: int) -> dict[str, NumberType]:\n",
    "        seq, candidates, labels = batch\n",
    "        scores: torch.Tensor = self(seq)\n",
    "        scores = scores[:, -1, :]\n",
    "        scores = scores.gather(1, candidates)\n",
    "        metrics = recalls_and_ndcgs_for_ks(scores, labels, self.metric_ks)\n",
    "\n",
    "        self.log_dict(metrics, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, batch: tuple[torch.Tensor, torch.Tensor, torch.Tensor], batch_idx: int) -> dict[str, NumberType]:\n",
    "        seq, candidates, labels = batch\n",
    "        scores: torch.Tensor = self(seq)\n",
    "        scores = scores[:, -1, :]\n",
    "        scores = scores.gather(1, candidates)\n",
    "        metrics = recalls_and_ndcgs_for_ks(scores, labels, self.metric_ks)\n",
    "\n",
    "        self.log_dict(metrics, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        return metrics\n",
    "\n",
    "    def validation_epoch_end(self, outputs: list[dict[str, NumberType]]) -> None:\n",
    "        # outputs is a list of dicts\n",
    "        avg_metrics = {}\n",
    "        for metric in outputs[0].keys():\n",
    "            avg_metrics[metric] = torch.mean(torch.FloatTensor([x[metric] for x in outputs]))\n",
    "        \n",
    "        self.log_dict(avg_metrics, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "\n",
    "\n",
    "        # for metric in outputs[0].keys():\n",
    "        #     self.log(metric, torch.mean(torch.stack(\n",
    "        #         [output[metric] for output in outputs])), on_step=False, on_epoch=True, prog_bar=False)\n",
    "    \n",
    "    def test_epoch_end(self, outputs: list[dict[str, NumberType]]) -> None:\n",
    "        # outputs is a list of dicts\n",
    "        metric_names = outputs[0].keys()\n",
    "        avg_metrics = dict.fromkeys(metric_names, [])\n",
    "        for output in outputs:\n",
    "            for metric_name in metric_names:\n",
    "                avg_metrics[metric_name].append(output[metric_name])\n",
    "        \n",
    "        for metric_name in metric_names:\n",
    "            avg_metrics[metric_name] = torch.mean(torch.stack(avg_metrics[metric_name]))\n",
    "        \n",
    "        self.log_dict(avg_metrics, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        # for metric in outputs[0].keys():\n",
    "        #     self.log(metric, torch.mean(torch.stack(\n",
    "        #         [output[metric] for output in outputs])), on_step=False, on_epoch=True, prog_bar=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.args['optimizer'].lower() == 'adam':\n",
    "            return torch.optim.Adam(self.parameters(), lr=self.args['lr'], weight_decay=self.args['weight_decay'])\n",
    "        elif self.args['optimizer'].lower() == 'sgd':\n",
    "            return torch.optim.SGD(self.parameters(), lr=self.args['lr'], weight_decay=self.args['weight_decay'], momentum=self.args['momentum'])\n",
    "        else:\n",
    "            raise ValueError('Optimizer not supported')\n",
    "\n",
    "    def _create_state_dict(self):\n",
    "        return {\n",
    "            'model_state_dict': self.model.module.state_dict() if self.is_parallel else self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = BertTrainDataset(train, 100, 0.15, item_count+1, item_count, random.Random(0))\n",
    "        self.val_dataset = BertEvalDataset(train, val, 100, item_count+1, negative_test_samples)\n",
    "        self.test_dataset = BertEvalDataset(train, test, 100, item_count+1, negative_test_samples)\n",
    "\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return data_utils.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=128,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return data_utils.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=128,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return data_utils.DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=128,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'bert_max_len': 100,\n",
    "    'num_items': item_count,\n",
    "    'bert_num_blocks': 2,\n",
    "    'bert_num_heads': 4,\n",
    "    'bert_hidden_units': 256,\n",
    "    'bert_dropout': 0.1,\n",
    "    'model_init_seed': 42,\n",
    "    'bert_mask_prob': 0.15,\n",
    "    'metric_ks': [1, 5, 10, 20],\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.0,\n",
    "    'optimizer': 'Adam',\n",
    "}\n",
    "\n",
    "args = dotdict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type                    | Params\n",
      "--------------------------------------------------------\n",
      "0 | ce          | CrossEntropyLoss        | 0     \n",
      "1 | embedding   | BERTEmbedding           | 974 K \n",
      "2 | encoder     | TransformerEncoderLayer | 789 K \n",
      "3 | transformer | TransformerEncoder      | 1.6 M \n",
      "4 | out         | Linear                  | 952 K \n",
      "--------------------------------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "17.187    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Coding\\bst-lstr\\bst\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Coding\\bst-lstr\\bst\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "e:\\Coding\\bst-lstr\\bst\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1892: PossibleUserWarning: The number of training batches (48) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  70%|██████▉   | 67/96 [00:03<00:01, 19.64it/s, loss=6.12, v_num=26, train/ce=5.910, Recall@20=0.986, NDCG@20=0.663, Recall@10=0.912, NDCG@10=0.645, Recall@5=0.789, NDCG@5=0.604, Recall@1=0.389, NDCG@1=0.389]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Coding\\bst-lstr\\bst\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:653: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "model = BERT(args)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=100)\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 ('bst': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ca0676aa18c95b2f4ddbcd7227ae51f1b24dbd48c44e8096aadec2eff2cfaf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
